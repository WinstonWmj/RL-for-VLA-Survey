# RL for VLA 信息源收集

## 学术论文 (Research Papers)

### ArXiv 论文
1. **Improving Vision-Language-Action Model with Online Reinforcement Learning**
   - URL: https://arxiv.org/abs/2501.16664
   - 作者: Y Guo et al., 2025
   - 引用: 68次
   - 摘要: 探索如何通过强化学习进一步改进VLA模型

2. **A Survey on Reinforcement Learning of Vision-Language-Action Models for Robotic Manipulation**
   - URL: https://www.techrxiv.org/users/934012/articles/1366553-a-survey-on-reinforcement-learning-of-vision-language-action-models-for-robotic-manipulation
   - 作者: H Deng
   - 摘要: RL利用自我探索和结果驱动优化来增强VLA的OOD泛化能力

3. **VLA-RFT: Vision-Language-Action Reinforcement Fine-Tuning**
   - URL: https://openreview.net/forum?id=Jaut99EHeu
   - 日期: 2025年11月26日
   - 摘要: 引入VLA-RFT框架，利用数据驱动的世界模型作为可控模拟器

4. **VLA-R1: Enhancing Reasoning in Vision-Language-Action Models**
   - URL: https://arxiv.org/abs/2510.01623
   - 作者: A Ye et al., 2025
   - 引用: 6次
   - 摘要: VLA模型旨在统一感知、语言理解和动作生成

5. **VLA-RL: Towards Masterful and General Robotic Manipulation**
   - URL: https://arxiv.org/abs/2505.18719
   - 作者: G Lu et al., 2025
   - 引用: 59次
   - 摘要: 提出VLA-RL框架，利用在线强化学习改进预训练的自回归VLA

6. **SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning**
   - URL: https://openreview.net/forum?id=TQhSodCM4r
   - 摘要: 基于VeRL构建的RL框架，用于训练机器人操作任务的VLA模型

7. **A Vision-Language-Action-Critic Model for Robotic Real-World RL**
   - URL: https://arxiv.org/abs/2509.15937
   - 摘要: VLAC模型，基于InternVL构建，用于上下文任务进度预测和动作生成

8. **Co-RFT: Efficient Fine-tuning of VLA Models through Chunked Offline RL**
   - URL: https://arxiv.org/abs/2508.02219
   - 摘要: 通过离线强化学习结合动作分块来微调VLA模型

9. **What Can RL Bring to VLA Generalization? An Empirical Study**
   - URL: https://rlvla.github.io/
   - 摘要: RL在执行层面显著增强VLA泛化，在语义层面适度改进

10. **Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation**
    - URL: https://arxiv.org/abs/2510.21571
    - 作者: Q Li et al., 2025
    - 引用: 3次

### 中文学术论文
11. **视觉–语言–动作模型综述: 从前史到前沿**
    - URL: https://aas.net.cn/cn/article/doi/10.16383/j.aas.c250417
    - 作者: 张慧等
    - 期刊: 自动化学报, 2025
    - 引用: 1次

12. **多模态环境中的多智能体强化学习: 预训练大模型视角**
    - URL: https://yingwen.io/files/LLM-MARL.pdf
    - 作者: 温颖等, 2023
    - 引用: 4次

## 技术博客 (Technical Blogs)

1. **OpenVLA finetuning with online RL | Haonan's blog**
   - URL: https://www.haonanyu.blog/post/openvla_rl/
   - 日期: 2025年7月6日
   - 内容: 深入技术细节，使用稀疏RL奖励微调OpenVLA

2. **Foundation Models for Robotics: Vision-Language-Action**
   - URL: https://rohitbandaru.github.io/blog/Foundation-Models-for-Robotics-VLA/
   - 日期: 2025年9月28日
   - 内容: 基础LLM如何适配机器人应用

3. **From Generalists to Specialists: A Case for Real-World RL in Robot Manipulation**
   - URL: https://rasc.usc.edu/blog/from-generalists-to-specialists-a-case-for-real-world-rl-in-robot-manipulation/
   - 日期: 2025年9月24日
   - 内容: 微调大型模型的案例研究

4. **Building VLA models from scratch — II | by Keivalya Pandya**
   - URL: https://medium.com/@keivalyap/building-vla-models-from-scratch-ii-0180020dbc85
   - 平台: Medium
   - 内容: VLA架构的深入技术部分

5. **Embodied AI Blog Series, Part 1: Getting Started with AWS**
   - URL: https://aws.amazon.com/blogs/spatial/embodied-ai-blog-series-part-1/
   - 平台: AWS Blog
   - 内容: 云端机器人学习栈

6. **Comprehensive Guide to Reinforcement Learning in Hugging Face**
   - URL: https://huggingface.co/blog/ProCreations/guide-to-rl
   - 日期: 2025年6月9日
   - 平台: Hugging Face Blog
   - 内容: 多模态RL通过VLA模型的进展

7. **Paper notes: Improving Vision-Language-Action Model with Online RL**
   - URL: https://itcanthink.substack.com/p/paper-notes-improving-vision-language
   - 平台: Substack
   - 内容: 论文笔记和分析

8. **Interesting Directions in Vision-Language-Action Model Research**
   - URL: https://itcanthink.substack.com/p/interesting-directions-in-vision
   - 日期: 1个月前
   - 平台: Substack
   - 内容: VLA模型与强化学习结合的研究方向

9. **A VLA that Learns from Experience | Physical Intelligence**
   - URL: https://www.physicalintelligence.company/blog/pistar06
   - 日期: 2025年11月17日
   - 内容: Recap方法（RL with Experience & Corrections）

10. **GR-RL Framework | ByteDance**
    - URL: https://seed.bytedance.com/en/gr_rl
    - 日期: 2025年12月2日
    - 内容: 将通用VLA策略转变为高能力专家的框架

## 中文博客和讨论

1. **一种基于在线强化学习的自动驾驶视觉-语言-动作模型**
   - URL: https://zhuanlan.zhihu.com/p/1989917011317379380
   - 平台: 知乎
   - 日期: 17小时前
   - 内容: 模仿学习+在线强化学习两阶段训练

2. **强化学习赋能视觉-语言-动作模型：进展、机制与前景综述**
   - URL: https://cloud.tencent.com/developer/article/2552890
   - 平台: 腾讯云开发者社区
   - 日期: 2025年8月7日
   - 内容: VLA模型作为具身智能核心范式的综述

3. **SimpleVLA-RL：通过强化学习扩展视觉语言动作模型的训练**
   - URL: https://www.chatpaper.ai/zh/paper/e3e2543f-7ce1-4c1f-87a5-59be902d0f7a
   - 平台: ChatPaper
   - 内容: 高效RL框架解决数据稀缺和泛化问题

4. **端到端VLA新范式！ReinboT：利用强化学习增强机器人视觉**
   - URL: https://blog.csdn.net/cv_autobot/article/details/148347741
   - 平台: CSDN
   - 日期: 2025年5月26日
   - 内容: VLA技术在自动驾驶和机器人领域的应用

## Reddit 讨论

1. **I built a tiny Vision-Language-Action (VLA) model from scratch**
   - URL: https://www.reddit.com/r/reinforcementlearning/comments/1parv6w/i_built_a_tiny_visionlanguageaction_vla_model/
   - 平台: Reddit r/reinforcementlearning
   - 日期: 1个月前
   - 内容: 从零构建VLA系统的实验

## GitHub 资源

1. **Awesome RL-VLA for Robotic Manipulation**
   - URL: https://github.com/Denghaoyuan123/Awesome-RL-VLA
   - 内容: RL-VLA论文和资源精选列表

2. **OpenVLA: An open-source vision-language-action model**
   - URL: https://github.com/openvla/openvla
   - 内容: 训练和微调VLA模型的简单可扩展代码库

## 项目网站

1. **What Can RL Bring to VLA Generalization?**
   - URL: https://rlvla.github.io/
   - 内容: 实证研究项目网站

2. **OpenVLA: An Open-Source Vision-Language-Action Model**
   - URL: https://openvla.github.io/
   - 内容: 7B参数开源VLA模型


---

## 核心论文详细内容

### 1. Improving Vision-Language-Action Model with Online Reinforcement Learning (ICRA 2025)

**作者**: Yanjiang Guo, Jianke Zhang, Xiaoyu Chen, Xiang Ji, Yen-Jen Wang, Yucheng Hu, Jianyu Chen

**核心贡献**:
- 提出 **iRe-VLA 框架**，在强化学习和监督学习之间迭代，有效改进VLA模型
- 解决直接将在线RL应用于大型VLA模型的两大挑战：
  1. 训练不稳定性严重影响大型模型性能
  2. 计算负担超出大多数本地机器能力
- 利用RL的探索优势同时保持监督学习的稳定性
- 在两个模拟基准和真实世界操作套件中验证有效性

**研究背景**:
- 近期研究通过监督微调(SFT)将大型视觉-语言模型(VLMs)集成到低级机器人控制中
- VLA模型虽然强大，但如何在与环境交互过程中改进这些大型模型仍是开放问题
- RL是大型模型常用的微调技术

**发表信息**: 已被ICRA 2025接受


### 2. VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning

**作者**: Guanxing Lu, Wenkai Guo, Chubin Zhang, Yuheng Zhou, Haonan Jiang, Zifeng Gao, Yansong Tang, Ziwei Wang

**核心贡献**:
- 提出 **VLA-RL 框架**，利用在线强化学习改进预训练的自回归VLA模型
- 引入轨迹级RL公式用于自回归VLA训练，将通用机器人操作轨迹建模为多模态多轮对话
- 微调预训练视觉-语言模型作为机器人过程奖励模型，解决稀疏奖励挑战
- 识别多个实现发现以提高稳定性和效率：
  - 课程选择策略
  - GPU平衡的向量化环境
  - 批量解码
  - Critic预热

**实验结果**:
- 使OpenVLA-7B在LIBERO的40个挑战性机器人操作任务上超越最强微调基线4.5%
- 甚至匹配π0-FAST等先进商业模型的性能
- 观察到VLA-RL受益于增加的测试时间优化，表明机器人领域推理扩展定律的早期迹象

**研究背景**:
- 高容量VLA模型通过模仿人类演示在机器人操作任务上表现出色
- 但利用有限访问状态的离线数据会导致分布外场景执行失败
- 基于探索的方法可以在测试时改进在线收集的数据

**提交日期**: 2025年5月24日


### 3. What Can RL Bring to VLA Generalization? An Empirical Study (NeurIPS 2025)

**作者**: Jijia Liu, Feng Gao, Bingwen Wei, Xinlei Chen, Qingmin Liao, Yi Wu, Chao Yu, Yu Wang
**机构**: 清华大学、上海期智研究院、中关村研究院

**核心研究问题**:
评估强化学习(RL)微调与监督微调(SFT)对视觉-语言-动作(VLA)模型泛化能力的影响

**主要发现**:
在分布外测试中，RL对VLA泛化的影响：
- **执行(Execution)维度**: 显著增强泛化能力
- **语义(Semantics)维度**: 适度改进泛化能力
- **视觉(Vision)维度**: 与SFT表现相当

**方法论**:

1. **RL算法比较**: 评估了PPO、GRPO和DPO三种代表性RL算法
   - PPO持续优于GRPO和DPO
   - 非平稳动态使GRPO不稳定
   - 稀疏奖励和分布偏移限制了DPO

2. **PPO设计因素**:
   - 共享actor-critic骨干网络：节省45% VRAM，训练速度提高53%
   - VLA预热：收敛所需环境步数减少约50%
   - 最小PPO轮次：减少实际时间，保持样本效率

3. **泛化维度定义**:
   - **视觉**: 前景和背景变化、图像级动态噪声
   - **语义**: 未见物体、容器、指令措辞变化及新任务
   - **执行**: 物体和容器初始位置变化、机器人初始姿态

**实验结果**:
- 数据规模方面：SFT在16k演示时饱和，RL在收敛时与SFT-16k在训练设置中表现相当，但在未见物体和桌子上表现好42.6%
- 基于OpenVLA模型进行实验验证

**项目资源**:
- 论文: https://arxiv.org/abs/2505.19789
- 代码: https://github.com/gen-robot/RL4VLA
- 模型: https://huggingface.co/collections/gen-robot/rlvla-684bc48aa6cf28bac37c57a2

**会议**: 已被NeurIPS 2025接受


### 4. π*0.6: A VLA that Learns from Experience (Physical Intelligence)

**发布日期**: 2025年11月17日
**机构**: Physical Intelligence
**论文**: π*0.6.pdf

**核心方法: RECAP (RL with Experience & Corrections via Advantage-conditioned Policies)**

RECAP实现了三个学习步骤：
1. 用人类演示训练机器人
2. 通过纠正进行指导
3. 从自主经验中改进

**技术创新**:
- 从"坏"的经验数据中提取好的训练信号的两种方式：
  1. **指导纠正**: 专家展示如何修复错误或做得更好
  2. **强化学习**: 机器人根据整体结果判断行为的好坏，迭代学习执行好的行为并避免坏的行为

- **信用分配挑战**: 理解哪些动作导致好结果，哪些导致坏结果
  - 训练价值函数来预测特定情况的好坏程度
  - 通过价值函数的变化来确定动作的好坏

**实验结果**:
- 在最困难的任务上，吞吐量提高超过2倍
- 失败率降低2倍或更多
- 实际应用场景：
  - 全天制作浓缩咖啡饮料（早上5:30到晚上11:30）
  - 在新家中连续数小时折叠50种不同的新衣物
  - 在真实工厂中组装和标记59个用于巧克力包装的盒子

**核心问题解决**:
- 为什么模仿学习不够？
  - VLA在控制机器人时会犯小错误
  - 错误会产生与训练数据不同的情况
  - 导致复合错误和最终失败
- RECAP通过使用VLA自己行为的额外数据来解决，训练它修复实际犯的错误


### 5. OpenVLA finetuning with online RL (Haonan Yu's Blog)

**发布日期**: 2025年7月6日
**作者**: Haonan Yu
**阅读时间**: 18分钟

**核心目标**:
使用在线(on-policy) RL微调OpenVLA，重点是改进OpenVLA的模型使其"准备好"用于现有RL算法

**选择的RL算法**:
- REINFORCE
- PPO (虽然严格来说是off-policy，但其流程具有on-policy风格)

**技术挑战与解决方案**:

**问题1: 批量动作预测**
- 批量rollout是on-policy RL的关键特性
- OpenVLA原生不支持批量生成
- 原始代码限制batch size为1
- **解决方案**: 
  - 移除batch size限制
  - 修复position_ids问题，确保padding tokens不影响实际tokens的位置ID
  - OpenVLA采用右填充(right padding)
  - 训练时动作tokens与指令tokens连接后再填充
  - 生成时需要先填充指令tokens再预测动作tokens

**关键技术细节**:
- Prismatic使用Llama2作为语言模型
- position_ids默认为None时会被硬编码为连续整数序列
- 需要手动提供position_ids以确保padding tokens不影响非padding tokens的位置ID

**相关代码库**:
- OpenVLA官方代码库
- OpenVLA-mini
- OpenVLA-OFT

**技术深度**: 深入探讨了OpenVLA架构的实现细节和RL微调的具体技术问题


### 6. Awesome RL-VLA for Robotic Manipulation (GitHub资源库)

**仓库**: https://github.com/Denghaoyuan123/Awesome-RL-VLA
**Stars**: 387 | **Forks**: 9 | **Watching**: 3
**贡献者**: Denghaoyuan123, KevinGuo07
**最新发布**: v0.1.0 (2025年11月25日)

**综述论文**: "A Survey on Reinforcement Learning of Vision-Language-Action Models for Robotic Manipulation"
- 发布平台: TechRxiv (2025年11月)
- URL: https://doi.org/10.36227/techrxiv.176531955.54563920/v1

**RL-VLA训练范式分类**:

1. **离线RL-VLA (Offline RL-VLA)**
   - 在预收集的静态数据集上训练VLA模型
   - 适用于高风险或资源受限的部署场景
   - 关键研究方向：数据利用、目标修改

2. **在线RL-VLA (Online RL-VLA)**
   - 通过持续环境交互实现交互式策略学习
   - 为预训练VLA提供自适应闭环控制能力
   - 关键研究方向：策略优化、样本效率、主动探索、训练稳定性、基础设施

3. **测试时RL-VLA (Test-time RL-VLA)**
   - 在部署期间通过轻量级更新适应行为
   - 解决实际场景中完整模型微调的高昂成本
   - 关键适应机制：价值引导、记忆缓冲引导、规划引导适应

**论文统计**:
- **离线RL-VLA**: 9篇论文 (2023.10-2025.11)
  - 代表方法: Q-Transformer, PAC, ReinboT, π*0.6, RECAP等
- **在线RL-VLA**: 29篇论文 (2024.09-2025.11)
  - 代表方法: FLaRe, PA-RL, iRe-VLA, VLA-RL, SimpleVLA-RL, VLAC等
- **离线+在线RL-VLA**: 6篇论文 (2025.04-2025.12)
  - 代表方法: ConRFT, SRPO, GR-RL, STARE-VLA等
- **测试时RL-VLA**: 7篇论文 (2024.10-2025.12)
  - 代表方法: V-GPS, Hume, VLA-Reasoner, TACO等

**动作优化策略分类**:
- **自回归VLA (Autoregressive)**: 在token级别优化动作
- **扩散VLA (Diffusion)**: 优化整个动作序列
- **流匹配VLA (Flow-matching)**: 优化连续动作流

**基础VLA模型**:
- OpenVLA系列 (OpenVLA, OpenVLA-OFT, OpenVLA-mini)
- π系列 (π₀, π₀.₅, π₀.₆, π₀-Fast)
- Octo系列
- 其他: SPOC, ReinboT, VLAC, Hume等

**RL算法分类**:
- Model-free: PPO, GRPO, DPO, REINFORCE, SAC, TD3, CQL等
- Model-based: 基于世界模型的方法
- 奖励类型: 稀疏奖励(S)、密集奖励(D)
